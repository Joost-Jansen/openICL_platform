# ICL Platform

## Overview
This repository houses the ICL Platform, an application designed to enrich the OpenICL framework through a comprehensive visual interface. Our platform stands out by facilitating in-depth analysis and experimentation with In-Context Learning (ICL) techniques across a diverse array of Large Language Models (LLMs), tasks, and datasets. We introduce CDTabuRetriever, a sample selection method that marries similarity-based retrieval with model-based confidence assessments, offering a nuanced approach to enhancing ICL's efficiency and effectiveness.

This [research](Paper.pdf) is completed under the Natural Language Processing course by TU Delft.

## Key Features
- **Visual Interface**: A user-friendly and intuitive interface for experimenting with and evaluating various ICL configurations.
- **CDTabuRetriever**: Our novel contribution to sample selection methods, optimizing for both similarity and confidence to improve ICL outcomes.
- **Extensive Customization**: Flexibility in configuring LLMs, tasks, and datasets for tailored experimentation.
- **Comprehensive Analysis Tools**: Tools for detailed analysis of ICL techniques, facilitating a deeper understanding and enhanced performance.

## Getting Started
To dive into the ICL Platform, begin by setting up the environment as outlined in the backend and frontend directories. Detailed instructions can be found in their respective `README.md` files.

## How to Use
1. **Setup**: Follow the setup instructions for both the backend and frontend components.
2. **Experimentation**: Utilise the visual interface to select models, tasks, datasets, and configure the CDTabuRetriever according to your research needs.
3. **Analysis**: Engage with the analysis tools to evaluate the impact of your configurations on ICL performance.

## Contributors
- Markus Trasberg
- Nazariy Lonyuk
- Joost Jansen
- Gijs Admiraal

*Date: June 2023*

For further details on our research and methodology, refer to our paper titled "Effective Sample Selection for In-context Learning and Analysis of Output Quality for Downstream Fine-tuning." (paper.pdf)

## Acknowledgements
This project was conducted under the guidance of the faculty at TU Delft, within the scope of the Natural Language Processing course. 
