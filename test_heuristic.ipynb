{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openicl import (DatasetReader, PromptTemplate, \n",
    "                     ZeroRetriever, RandomRetriever, BM25Retriever,\n",
    "                     GenInferencer, PPLInferencer)\n",
    "from openicl.icl_dataset_reader import load_dataset\n",
    "import pandas as pd\n",
    "from accelerate import Accelerator\n",
    "from QPKTabuRetriever import QPKTabuRetriever\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"roberta-large\", \"gpt2-large\"]\n",
    "TASKS = ['question-answering', 'sentiment-analysis']\n",
    "DATASET_NAMES = {\n",
    "    'question-answering':['commonsense_qa','tasksource/bigbench'],\n",
    "    'sentiment-analysis':['imdb', 'gpt3mix/sst2']\n",
    "}\n",
    "RETRIEVERS = ['zero', 'random', 'bm25', 'qkp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmqa_pre_process(example):\n",
    "    for i in range(5):\n",
    "        example[chr(ord('A') + i)] = example['choices']['text'][i]\n",
    "    return example\n",
    "\n",
    "def bb_pre_process(example):\n",
    "    for i in range(3):\n",
    "        example[chr(ord('A') + i)] = example['multiple_choice_targets'][i]\n",
    "    example['multiple_choice_scores'] = chr(ord('A') + np.where(np.array(example['multiple_choice_scores']) == 1)[0][0])\n",
    "    example['context'] = \"Disambiguation\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dataset(name):\n",
    "    if name == 'commonsense_qa':\n",
    "        dataset = load_dataset(name, split='train')\n",
    "        dataset = dataset.train_test_split(test_size=10, train_size=20, shuffle=True)\n",
    "        dataset = dataset.map(cmqa_pre_process)\n",
    "        dataset = dataset.rename_column(\"question_concept\",\"context\")\n",
    "        dataset = dataset.rename_column(\"answerKey\",\"answer\")\n",
    "        input_cols = [\"question\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "        return DatasetReader(dataset=dataset, input_columns=input_cols, output_column=\"answer\")\n",
    "    elif name == 'tasksource/bigbench':\n",
    "        dataset = load_dataset(name, 'disambiguation_qa', split='train')\n",
    "        dataset = dataset.train_test_split(test_size=10, train_size=20, shuffle=True)\n",
    "        dataset = dataset.map(bb_pre_process)\n",
    "        dataset = dataset.rename_column(\"multiple_choice_scores\",\"answer\")\n",
    "        dataset = dataset.rename_column(\"inputs\",\"question\")\n",
    "        input_cols = [\"question\", \"context\", \"A\", \"B\", \"C\"]\n",
    "        return DatasetReader(dataset=dataset, input_columns=input_cols, output_column=\"answer\")\n",
    "    elif name == 'imdb' or name == 'gpt3mix/sst2':\n",
    "        dataset = load_dataset(name, split='train')\n",
    "        dataset = dataset.train_test_split(test_size=10, train_size=20, shuffle=True)\n",
    "        return DatasetReader(dataset=dataset, input_columns=[\"text\"], output_column=\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATES = {\n",
    "    'commonsense_qa':PromptTemplate(\n",
    "        {\n",
    "            'A': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans1>\",\n",
    "            'B': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans2>\",\n",
    "            'C': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans3>\",\n",
    "            'D': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans4>\",\n",
    "            'E': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans5>\",\n",
    "        },\n",
    "        {'question':'</Q>', 'A': '</Ans1>', 'B': '</Ans2>', 'C': '</Ans3>', 'D': '</Ans4>', 'E': '</Ans5>'},\n",
    "        ice_token='</E>' \n",
    "    ),\n",
    "    'tasksource/bigbench':PromptTemplate(\n",
    "        {\n",
    "            'A': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans1>\",\n",
    "            'B': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans2>\",\n",
    "            'C': \"</E>Answer the following question:\\n</Q>\\nAnswer: </Ans3>\"\n",
    "        },\n",
    "        {'question':'</Q>', 'A': '</Ans1>', 'B': '</Ans2>', 'C': '</Ans3>'},\n",
    "        ice_token='</E>' \n",
    "    ),\n",
    "    'imdb':PromptTemplate({\n",
    "            0: '</E>Positive Movie Review: \\\"<X>\\\"', \n",
    "            1: '</E>Negative Movie Review: \\\"<X>\\\"',\n",
    "        }, column_token_map={'text' : '<X>'}, \n",
    "        ice_token='</E>'\n",
    "    ),\n",
    "    'gpt3mix/sst2':PromptTemplate({\n",
    "            0: '</E>Positive Movie Review: \\\"<X>\\\"', \n",
    "            1: '</E>Negative Movie Review: \\\"<X>\\\"',\n",
    "        }, column_token_map={'text' : '<X>'}, \n",
    "        ice_token='</E>'\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_retriever(retr_name, data, model, task, ice_num, accelerator):\n",
    "    if retr_name == 'zero':\n",
    "        return ZeroRetriever(data)\n",
    "    elif retr_name == 'random':\n",
    "        return RandomRetriever(data, ice_num=ice_num, accelerator=accelerator)\n",
    "    elif retr_name == 'bm25':\n",
    "        return BM25Retriever(data, ice_num=ice_num, accelerator=accelerator)\n",
    "    elif retr_name == 'qkp':\n",
    "        return QPKTabuRetriever(data, model=model, task=task, ice_num=ice_num, accelerator=accelerator)\n",
    "    else:\n",
    "        raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/nlonyuk/.cache/huggingface/modules/datasets_modules/datasets/gpt3mix--sst2/90167692658fa4abca2ffa3ede1a43a71e2bf671078c5c275c64c4231d5a62fa (last modified on Fri Jun  2 10:43:35 2023) since it couldn't be found locally at gpt3mix/sst2., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset sst2 (/home/nlonyuk/.cache/huggingface/datasets/gpt3mix___sst2/default/0.0.0/90167692658fa4abca2ffa3ede1a43a71e2bf671078c5c275c64c4231d5a62fa)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-06-14 09:02:48,472] [openicl.icl_inferencer.icl_ppl_inferencer] [INFO] Calculating PPL for prompts labeled '0'\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.20it/s]\n",
      "[2023-06-14 09:02:53,022] [openicl.icl_inferencer.icl_ppl_inferencer] [INFO] Calculating PPL for prompts labeled '1'\n",
      "100%|██████████| 10/10 [00:05<00:00,  1.87it/s]\n",
      "Using the latest cached version of the module from /home/nlonyuk/.cache/huggingface/modules/datasets_modules/datasets/gpt3mix--sst2/90167692658fa4abca2ffa3ede1a43a71e2bf671078c5c275c64c4231d5a62fa (last modified on Fri Jun  2 10:43:35 2023) since it couldn't be found locally at gpt3mix/sst2., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset sst2 (/home/nlonyuk/.cache/huggingface/datasets/gpt3mix___sst2/default/0.0.0/90167692658fa4abca2ffa3ede1a43a71e2bf671078c5c275c64c4231d5a62fa)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'model':[],\n",
    "    'task':[],\n",
    "    'dataset':[],\n",
    "    'retriever':[],\n",
    "    'accuracy_mean':[],\n",
    "    'accuracy_std':[],\n",
    "    'predictions':[],\n",
    "    'inputs':[]\n",
    "}\n",
    "\n",
    "accelerator = Accelerator()\n",
    "ice_num = 5\n",
    "reps = 3\n",
    "\n",
    "for model in MODELS:\n",
    "    for task in TASKS:\n",
    "        for dataset_name in DATASET_NAMES[task]:\n",
    "            for retr_name in RETRIEVERS:\n",
    "                print(retr_name)\n",
    "                accuracies = list()\n",
    "                all_predictions = list()\n",
    "                all_inputs = list()\n",
    "                results['model'].append(model)\n",
    "                results['task'].append(task)\n",
    "                results['dataset'].append(dataset_name)\n",
    "                results['retriever'].append(retr_name)\n",
    "\n",
    "                for _ in range(reps):\n",
    "                    data = select_dataset(dataset_name)\n",
    "                    retriever = select_retriever(retr_name, data, model, task, ice_num, accelerator)\n",
    "                    inferencer = PPLInferencer(model_name=model, accelerator=accelerator)\n",
    "                    ice_template = TEMPLATES[dataset_name] \n",
    "                    predictions = inferencer.inference(retriever, ice_template=ice_template)\n",
    "                    all_predictions.append(predictions)\n",
    "                    all_inputs.append(retriever.test_ds[retriever.dataset_reader.input_columns[0]])\n",
    "                    accuracies.append(np.sum(np.sum(np.array(retriever.test_ds[retriever.dataset_reader.output_column]) == np.array(predictions))))\n",
    "                \n",
    "                results['accuracy_mean'].append(np.mean(accuracies))\n",
    "                results['accuracy_std'].append(np.std(accuracies))\n",
    "                results['predictions'].append(all_predictions)\n",
    "                results['inputs'].append(all_inputs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x, ys, title, labels, savefile):\n",
    "    configs = ['g*-', 'bo-', 'r+-']\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    # ax.set_xscale('log')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x)\n",
    "    ax.set_title(title)\n",
    "    plt.grid()\n",
    "    for idx, y in enumerate(ys):\n",
    "        plt.plot(x, y, configs[idx], label=labels[idx])\n",
    "        plt.legend()\n",
    "    plt.savefig(f'{savefile}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in TASKS:\n",
    "    df_instance = df_results[df_results['task'] == task]\n",
    "    evals = [df_instance[df_instance['retriever'] == retr_name]['evals'] for retr_name in RETRIEVERS]\n",
    "\n",
    "    plot(DATASET_NAMES[task], evals, f'Mean acccuracy for task: {task}', RETRIEVERS, f'{task}_evals')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
